{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAI643 - Artificial Intelligence in Medicine\n",
    "\n",
    "Project Assignment 1 - Spring Semester 2024\n",
    "\n",
    "Student Name:    \n",
    "Christina Ioanna Saroglaki   \n",
    "Jianlin Ye \n",
    "\n",
    "UCY Email:     \n",
    "saroglaki.christina-ioanna@ucy.ac.cy    \n",
    "jye00001@ucy.ac.cy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "np.set_printoptions(formatter={'float':\"{:6.5g}\".format})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df = pd.read_csv(\"risk_factors_cervical_cancer.csv\")\n",
    "\n",
    "# Only keep the \"Biopsy\" column as the target variable\n",
    "risk_factor_df = risk_factor_df.drop(columns=[\"Hinselmann\",\"Schiller\",\"Citology\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranformed all the numeric values into  the correct numeric type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df = risk_factor_df.apply(pd.to_numeric, errors = \"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset to features and target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = risk_factor_df.iloc[:,:-4]\n",
    "dep_df = risk_factor_df.iloc[:,-4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values\n",
    "\n",
    "During the preliminary analysis identified that the features “STDs: Time since first diagnosis” and “STDs: Time since last diagnosis” were filled with NaN values to about 92%. Because of the high percentage of missing values, it impractical to either eliminate those observations or fill the missing data with the mean of the existing data. Consequently, these features were excluded from the dataset for the development of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df = risk_factor_df.drop(columns=[\"STDs: Time since first diagnosis\", \"STDs: Time since last diagnosis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining columns, we managed the missing values depending on the column. In more detail, if the column contained binary values (0,1) then the row containing the missing value was deleted. Otherwise, the missing value was replaced with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------- Handling Missing Values ---------------------------\")\n",
    "print(\"----------------------------------- BEFORE -----------------------------------\")\n",
    "print(\"Number of rows before filling missing values: \", len(risk_factor_df))\n",
    "\n",
    "# Display the number of missing values before filling\n",
    "print(\"\\nNumber of missing values per column before filling:\")\n",
    "print(risk_factor_df.isnull().sum())\n",
    "\n",
    "# Fill missing values depending on the column\n",
    "for col in risk_factor_df.columns:\n",
    "    # If the column has more than 3 unique values, fill with mean of the column\n",
    "    if risk_factor_df[col].nunique() > 3:\n",
    "        risk_factor_df[col].fillna(risk_factor_df[col].median(), inplace=True)\n",
    "    \n",
    "# Drop rest NaN containing rows\n",
    "risk_factor_df=risk_factor_df.dropna()\n",
    "risk_factor_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n----------------------------------- AFTER -----------------------------------\")\n",
    "print(\"Number of rows after filling missing values: \", len(risk_factor_df))\n",
    "\n",
    "# Display the number of missing values after filling\n",
    "print(\"\\nNumber of missing values per column after filling:\")\n",
    "print(risk_factor_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Rows\n",
    "\n",
    "Removing duplicate rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------- Duplicate Rows -----------------------------------\")\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = risk_factor_df.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "\n",
    "if num_duplicates == 0:\n",
    "    print(\"No duplicate rows found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {num_duplicates} duplicate rows in the dataset.\\n\")\n",
    "\n",
    "    # Display the duplicate rows indexes (if any)\n",
    "    print(\"Duplicate rows indexes: {}\\n\".format(risk_factor_df[duplicate_rows].index.values))\n",
    "\n",
    "    # Removing duplicate rows\n",
    "    print(\"----------------------------- Removing Duplicates ----------------------------\")\n",
    "    print(\"----------------------------------- BEFORE -----------------------------------\")\n",
    "    print(\"Number of rows before removing duplicates: \", len(risk_factor_df))\n",
    "\n",
    "    risk_factor_df.drop_duplicates(inplace=True)\n",
    "    risk_factor_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"\\n----------------------------------- AFTER -----------------------------------\")\n",
    "    print(\"Number of rows after removing duplicates: \", len(risk_factor_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Outliers\n",
    "\n",
    "IQR (Inter Quartile Range) Inter Quartile Range approach to finding the outliers is the most commonly used and most trusted approach used in the research field. We utilised IQR to identify and remove outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define Unique Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function finding the unique values of each column in the dataframe\n",
    "def find_unique_values_df(feat: pd.DataFrame):\n",
    "    return {col: feat[col].unique() for col in feat}\n",
    "\n",
    "print(\"----------------------------------- Unique Values -----------------------------------\")    \n",
    "# Unique Values\n",
    "unique_vals = find_unique_values_df(risk_factor_df)\n",
    "\n",
    "# Print unique values for each column\n",
    "for col, col_unique_vals in unique_vals.items():\n",
    "    print(f\"{col}:\")\n",
    "    print(col_unique_vals)\n",
    "    print(risk_factor_df[col].dtypes)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(col, indices):\n",
    "    obs = risk_factor_df[col].iloc[indices]\n",
    "    unique_items, counts = np.unique(obs, return_counts=True)\n",
    "    unique_items, counts = unique_items[::-1], counts[::-1]\n",
    "\n",
    "    values_to_delete = unique_items[counts < 2 ]\n",
    "    return values_to_delete\n",
    "\n",
    "def delete_outliers(col, to_delete):\n",
    "    if (to_delete.size != 0):\n",
    "        rows_to_del = risk_factor_df.loc[risk_factor_df[col].isin(to_delete)].index.values.tolist()\n",
    "\n",
    "        # Remove rows\n",
    "        risk_factor_df.drop(rows_to_del, inplace=True)\n",
    "        risk_factor_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Identify non-binary columns\n",
    "non_binary_cols = [col for col, vals in unique_vals.items() if len(vals) > 2]\n",
    "\n",
    "for col in non_binary_cols:\n",
    "\n",
    "    # IQR cannot be applied to columns with median 0\n",
    "    if (risk_factor_df[col].median() != 0):\n",
    "\n",
    "        # Plot values distribution\n",
    "        out_dist = px.histogram(risk_factor_df, x=col,\n",
    "            marginal=\"box\",\n",
    "            color_discrete_sequence= px.colors.sequential.thermal)\n",
    "        out_dist.update_layout(bargap=0.2,\n",
    "            width=700)\n",
    "        out_dist.show()\n",
    "\n",
    "        Q3, Q1 = np.percentile(risk_factor_df[col], [75 ,25])\n",
    "        IQR = Q3-Q1\n",
    "\n",
    "        upper = Q3+(1.5*IQR)\n",
    "        lower = Q1-(1.5*IQR)\n",
    "\n",
    "        print(col)\n",
    "        print(\"median: {}, upper fence: {}, lower fence: {}\".format(risk_factor_df[col].median(), upper, lower))\n",
    "\n",
    "        #Delete one occurrence observations outside the upper fence as outliers\n",
    "        upper_to_delete = find_outliers(col, np.where(risk_factor_df[col] > upper)[0])\n",
    "        delete_outliers(col, upper_to_delete)\n",
    "\n",
    "        \n",
    "        #Delete one occurrence observations outside the lower fence as outliers\n",
    "        lower_to_delete = find_outliers(col, np.where(risk_factor_df[col] < lower)[0])\n",
    "        delete_outliers(col, lower_to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal dataset size: {} cols, {} rows\".format(risk_factor_df.shape[1], risk_factor_df.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
