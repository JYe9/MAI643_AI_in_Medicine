{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAI643 - Artificial Intelligence in Medicine\n",
    "\n",
    "Project Assignment 1 - Spring Semester 2024\n",
    "\n",
    "Student Name:    \n",
    "Christina Ioanna Saroglaki   \n",
    "Jianlin Ye \n",
    "\n",
    "UCY Email:     \n",
    "saroglaki.christina-ioanna@ucy.ac.cy    \n",
    "jye00001@ucy.ac.cy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "np.set_printoptions(formatter={'float':\"{:6.5g}\".format})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "As per the authors, the chosen dataset focuses on indicators associated with the diagnosis of cervical cancer, encompassing various features such as demographic information, habits, and medical records​. In more detail, the data was gathered at \"Hospital Universitario de Caracas\" in Venezuela from a total of 858 patients​.\n",
    "\n",
    "C. J. Fernandes Kelwin and J. Fernandes, “Cervical cancer (Risk Factors),” UCI Machine \n",
    "Learning Repository. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df = pd.read_csv(\"risk_factors_cervical_cancer.csv\", \n",
    "            na_values=[\"?\"])\n",
    "\n",
    "print(\"----------------------------------- Information -----------------------------------\")\n",
    "risk_factor_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis of the dataset\n",
    "\n",
    "To gain a better understanding of the dataset, we conducted a preliminary analysis.\n",
    "### Missing Values\n",
    "\n",
    "First, we needed to find the volume of missing values contained in the dataset as well as the features that contained the largest amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------- Missing Values -----------------------------------\")\n",
    "missing_info = risk_factor_df.isnull().sum()\n",
    "total_nan = missing_info.sum()\n",
    "total_entries = risk_factor_df.size\n",
    "\n",
    "# Print total NaN values\n",
    "if (total_nan == 0):\n",
    "    print(\"\\nNo NaN values in the dataset.\")\n",
    "else:\n",
    "    print(\"\\nNaN values found in the dataset.\")\n",
    "\n",
    "    print(\"\\nTotal NaN values in dataset: {}/{}\".format(total_nan, total_entries))\n",
    "\n",
    "    # Sort columns by the number of missing values\n",
    "    nan_columns = missing_info.sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nTop 15 columns with missing values:\\n\")\n",
    "    for i, (col, count) in enumerate(nan_columns.head(15).items(), 1):\n",
    "        print(\"{:2}. {:35} : {:}\".format(i, col, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "total_figure = px.pie(values=[total_nan, total_entries-total_nan], names=[\"NaN values\", \"Valid Values\"],\n",
    "        color_discrete_sequence=px.colors.sequential.Aggrnyl,\n",
    "        title=\"Total NaN Values Distribution\",\n",
    "        width=550, height= 350)\n",
    "\n",
    "total_figure.update_layout(\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    title_x=0.5    \n",
    ")\n",
    "\n",
    "total_figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows containing NaN values\n",
    "total_rows = len(risk_factor_df)\n",
    "nan_rows = risk_factor_df.isna().any(axis=1).tolist().count(True)\n",
    "print(\"\\nTotal Rows containing NaN values in dataset: {}/{}\".format(nan_rows, total_rows))\n",
    "\n",
    "rows_fig=go.Figure(data=[go.Pie(labels=[\"Has NaN Values\",\"Is Filled\"],\n",
    "    values=[nan_rows, total_rows],\n",
    "    marker_colors=[px.colors.sequential.Agsunset[0], px.colors.sequential.Agsunset[1]])])\n",
    "\n",
    "rows_fig.update_layout(\n",
    "    title=\"NaN Containing Rows Distribution\",\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    title_x=0.5,\n",
    "    width=550, height= 350    \n",
    ")\n",
    "\n",
    "rows_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified that the features “STDs: Time since first diagnosis” and “STDs: Time since last diagnosis” were filled with NaN values of about 92%. Because of the high percentage, it was impractical to either eliminate the affected observations or fill the missing values with the mean of columns. Consequently, these features were excluded from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df.drop(columns=[\"STDs: Time since first diagnosis\", \"STDs: Time since last diagnosis\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the optimal performance of future models, we also set a missing value threshold of 10 per row. Any rows that exceeded this threshold were eliminated from the dataset because we determined they were missing significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows containing NaN values\n",
    "nan_rows = risk_factor_df.isna().any(axis=1).tolist().count(True)\n",
    "print(\"\\nTotal Rows containing NaN values in dataset: {}/{}\".format(nan_rows, total_rows))\n",
    "\n",
    "# Find rows that contain more than 10 NaN values\n",
    "rows_to_del = risk_factor_df[risk_factor_df.isna().sum(axis=1) > 10].index\n",
    "\n",
    "print(\"\\nRows containing >10 NaN values: {}/{}\".format(len(rows_to_del), total_rows))\n",
    "\n",
    "# Remove rows\n",
    "risk_factor_df.drop(rows_to_del, inplace=True)\n",
    "risk_factor_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "color_1 = [px.colors.sequential.Agsunset[0], px.colors.sequential.Agsunset[1]]\n",
    "color_2 = [px.colors.sequential.Agsunset[2], px.colors.sequential.Agsunset[3]]\n",
    "\n",
    "\n",
    "row_figure = make_subplots(1, 2, specs=[[{\"type\":\"domain\"}, {\"type\":\"domain\"}]],\n",
    "    subplot_titles=[\"Contain NaN Values\", \"Contain >10 NaN Values\"])\n",
    "\n",
    "row_figure.add_trace(go.Pie(labels=[\"Has NaN Values\",\"Is Filled\"],\n",
    "    values=[nan_rows, total_rows - nan_rows],\n",
    "    marker_colors=color_1,\n",
    "    pull=[0.1, 0]), 1, 1)\n",
    "\n",
    "row_figure.add_trace(go.Pie(labels=[\">10 NaN\", \"<10 NaN\"],\n",
    "    values=[len(rows_to_del), nan_rows - len(rows_to_del)],\n",
    "    marker_colors=color_2), 1, 2)\n",
    "\n",
    "row_figure.update_layout(title_text=\"Rows Containing NaN Values\",\n",
    "    width=650, height= 400,\n",
    "    title_x=0.5)\n",
    "\n",
    "row_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining columns, we managed the missing values depending on the column. In more detail, if the column contained binary values (0,1) then the row containing the missing value was deleted. Otherwise, the missing value was replaced with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------- Handling Missing Values ---------------------------\")\n",
    "print(\"----------------------------------- BEFORE -----------------------------------\")\n",
    "print(\"Number of rows before filling missing values: \", len(risk_factor_df))\n",
    "\n",
    "# Display the number of missing values before filling\n",
    "print(\"\\nNumber of missing values per column before filling:\")\n",
    "print(risk_factor_df.isnull().sum())\n",
    "\n",
    "# Fill missing values depending on the column\n",
    "for col in risk_factor_df.columns:\n",
    "    # If the column has more than 3 unique values, fill with mean of the column\n",
    "    if risk_factor_df[col].nunique() > 3:\n",
    "        risk_factor_df[col].fillna(risk_factor_df[col].median(), inplace=True)\n",
    "    \n",
    "# Drop rest NaN containing rows\n",
    "risk_factor_df=risk_factor_df.dropna()\n",
    "risk_factor_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n----------------------------------- AFTER -----------------------------------\")\n",
    "print(\"Number of rows after filling missing values: \", len(risk_factor_df))\n",
    "\n",
    "# Display the number of missing values after filling\n",
    "print(\"\\nNumber of missing values per column after filling:\")\n",
    "print(risk_factor_df.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Rows\n",
    "\n",
    "Following the missing value analysis, we examined if the dataset contained any duplicate rows and removed them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------- Duplicate Rows -----------------------------------\")\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = risk_factor_df.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "\n",
    "if num_duplicates == 0:\n",
    "    print(\"No duplicate rows found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {num_duplicates} duplicate rows in the dataset.\\n\")\n",
    "\n",
    "    # Display the duplicate rows indexes (if any)\n",
    "    print(\"Duplicate rows indexes: {}\\n\".format(risk_factor_df[duplicate_rows].index.values))\n",
    "\n",
    "    # Removing duplicate rows\n",
    "    print(\"----------------------------- Removing Duplicates ----------------------------\")\n",
    "    print(\"----------------------------------- BEFORE -----------------------------------\")\n",
    "    print(\"Number of rows before removing duplicates: \", len(risk_factor_df))\n",
    "\n",
    "    risk_factor_df.drop_duplicates(inplace=True)\n",
    "    risk_factor_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"\\n----------------------------------- AFTER -----------------------------------\")\n",
    "    print(\"Number of rows after removing duplicates: \", len(risk_factor_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concluded the first phase of the preliminary analysis. After managing all the missing values and duplicate rows, the dataset had 34 features and 708 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal dataset size: {} cols, {} rows\".format(risk_factor_df.shape[1], risk_factor_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding features\n",
    "\n",
    "Once the first part of the analysis was completed, we moved on to exploring the features and some statistical properties of the dataset. This would allow us to identify possible connections between the features as well as possible imbalances.\n",
    "\n",
    "#### Unique Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function finding the unique values of each column in the dataframe\n",
    "def find_unique_values_df(feat: pd.DataFrame):\n",
    "    return {col: feat[col].unique() for col in feat}\n",
    "\n",
    "print(\"----------------------------------- Unique Values -----------------------------------\")    \n",
    "# Unique Values\n",
    "unique_vals = find_unique_values_df(risk_factor_df)\n",
    "\n",
    "# Print unique values for each column\n",
    "for col, col_unique_vals in unique_vals.items():\n",
    "    print(f\"{col}:\")\n",
    "    print(col_unique_vals)\n",
    "    print(risk_factor_df[col].dtypes)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Values Distribution\n",
    "\n",
    "First, we analyzed the dataset's balance. As shown in the graph, the dataset has a large imbalance across all four target variables. This imbalance complicates model training and evaluation, and it should be handled during the preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCount(col, value):\n",
    "    return risk_factor_df[col].value_counts()[value]\n",
    "\n",
    "# Plot occurrences of each class in the dataset\n",
    "classes_df = pd.DataFrame(\n",
    "    [[\"Hinselmann\", getCount(\"Hinselmann\", 0), getCount(\"Hinselmann\", 1)],\n",
    "        [\"Schiller\", getCount(\"Schiller\", 0), getCount(\"Schiller\", 1)],\n",
    "        [\"Citology\", getCount(\"Citology\", 0), getCount(\"Citology\", 1)],\n",
    "        [\"Biopsy\", getCount(\"Biopsy\", 0), getCount(\"Biopsy\", 1)]],\n",
    "    columns =[\"Exam\", \"Healthy\", \"Cervical Cancer\"])\n",
    "\n",
    "\n",
    "balance_fig = px.histogram(classes_df, x=\"Exam\", y=[\"Healthy\", \"Cervical Cancer\"],\n",
    "    title=\"Class Distribution\",\n",
    "    labels={\n",
    "        \"value\":\"Occurrences\",\n",
    "        \"variable\": \"Result\"\n",
    "    },\n",
    "    barmode=\"group\",\n",
    "    text_auto=True,\n",
    "    color_discrete_sequence=px.colors.qualitative.Bold,\n",
    "    width=600)\n",
    "\n",
    "balance_fig.update_layout(\n",
    "    title_x=0.5    \n",
    ")\n",
    "\n",
    "balance_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Properties\n",
    "\n",
    "Moving on to the statistical properties of the dataset, we calculated the mean and standard deviation for each column. Columns with a standard deviation of 0 were omitted from the dataset because they did not add significant variability to the data since they contained the same value for all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = risk_factor_df.mean()\n",
    "std_df = risk_factor_df.std()\n",
    "\n",
    "# Print columns that have a standard deviation 0 (contain only one value)\n",
    "print(\"Columns containing 1 value: {}\\n\".format(std_df[std_df==0].index.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df.drop(columns=[\"STDs:cervical condylomatosis\", \"STDs:AIDS\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "mean_df = risk_factor_df.mean()\n",
    "std_df = risk_factor_df.std()\n",
    "\n",
    "statistic_fig = go.Figure(data=[go.Table(\n",
    "        header=dict(values=[\"Feature\", \"Mean\", \"Standard Deviation\"]),\n",
    "        cells=dict(values=[list(risk_factor_df.columns), mean_df.values, std_df.values],\n",
    "                    align=['left', 'center'],\n",
    "                    format=[\"\",\".2\"])\n",
    "    )\n",
    "])\n",
    "\n",
    "statistic_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Outliers\n",
    "\n",
    "IQR (Inter Quartile Range) Inter Quartile Range approach to finding the outliers is the most commonly used and most trusted approach used in the research field. We utilised IQR to identify and remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(col, indices):\n",
    "    obs = risk_factor_df[col].iloc[indices]\n",
    "    unique_items, counts = np.unique(obs, return_counts=True)\n",
    "    unique_items, counts = unique_items[::-1], counts[::-1]\n",
    "\n",
    "    values_to_delete = unique_items[counts < 2 ]\n",
    "    return values_to_delete\n",
    "\n",
    "def delete_outliers(col, to_delete):\n",
    "    if (to_delete.size != 0):\n",
    "        rows_to_del = risk_factor_df.loc[risk_factor_df[col].isin(to_delete)].index.values.tolist()\n",
    "\n",
    "        # Remove rows\n",
    "        risk_factor_df.drop(rows_to_del, inplace=True)\n",
    "        risk_factor_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Identify non-binary columns\n",
    "non_binary_cols = [col for col, vals in unique_vals.items() if len(vals) > 2]\n",
    "\n",
    "for col in non_binary_cols:\n",
    "\n",
    "    # IQR cannot be applied to columns with median 0\n",
    "    if (risk_factor_df[col].median() != 0):\n",
    "\n",
    "        # Plot values distribution\n",
    "        out_dist = px.histogram(risk_factor_df, x=col,\n",
    "            marginal=\"box\",\n",
    "            color_discrete_sequence= px.colors.sequential.thermal)\n",
    "        out_dist.update_layout(bargap=0.2,\n",
    "            width=700)\n",
    "        out_dist.show()\n",
    "\n",
    "        Q3, Q1 = np.percentile(risk_factor_df[col], [75 ,25])\n",
    "        IQR = Q3-Q1\n",
    "\n",
    "        upper = Q3+(1.5*IQR)\n",
    "        lower = Q1-(1.5*IQR)\n",
    "\n",
    "        print(col)\n",
    "        print(\"median: {}, upper fence: {}, lower fence: {}\".format(risk_factor_df[col].median(), upper, lower))\n",
    "\n",
    "        #Delete one occurrence observations outside the upper fence as outliers\n",
    "        upper_to_delete = find_outliers(col, np.where(risk_factor_df[col] > upper)[0])\n",
    "        delete_outliers(col, upper_to_delete)\n",
    "\n",
    "        \n",
    "        #Delete one occurrence observations outside the lower fence as outliers\n",
    "        lower_to_delete = find_outliers(col, np.where(risk_factor_df[col] < lower)[0])\n",
    "        delete_outliers(col, lower_to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal dataset size: {} cols, {} rows\".format(risk_factor_df.shape[1], risk_factor_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation with label\n",
    "\n",
    "Lastly we found the correlation between each of the features and each of the target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corr(target, col):\n",
    "    return risk_factor_df[target].corr(risk_factor_df[col])\n",
    "\n",
    "# Create dictionaries\n",
    "target_variables = [\"Hinselmann\", \"Schiller\", \"Citology\", \"Biopsy\"]\n",
    "correlations = {target: {} for target in target_variables}\n",
    "\n",
    "# Calculate correlations\n",
    "for target in target_variables:\n",
    "    target_corr = risk_factor_df.iloc[:, :-4].corrwith(risk_factor_df[target])\n",
    "    correlations[target] = dict(target_corr.abs().sort_values())\n",
    "    \n",
    "# Plot graphs\n",
    "for target in correlations:\n",
    "    target_df = pd.DataFrame.from_dict(correlations[target], orient=\"index\", columns=[\"Correlation\"])\n",
    "\n",
    "    target_fig = px.bar(target_df, x=\"Correlation\",\n",
    "        orientation='h',\n",
    "        title=\"Features & {} Correlations\".format(target),\n",
    "        labels={\n",
    "            \"index\": \"Features\"\n",
    "        },\n",
    "        width=900, height=700)\n",
    "    \n",
    "    target_fig.update_layout(\n",
    "        title_x=0.5    \n",
    "    )\n",
    "    \n",
    "    target_fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mai644",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "32bdd95e8c4ecada84a0073ec4e8d048a8aaf2397f6888f3b1d4c4db30935bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
