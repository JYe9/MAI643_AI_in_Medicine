{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAI643 - Artificial Intelligence in Medicine\n",
    "\n",
    "Project Assignment 1 - Spring Semester 2024\n",
    "\n",
    "Student Name:    \n",
    "Christina Ioanna Saroglaki   \n",
    "Jianlin Ye \n",
    "\n",
    "UCY Email:     \n",
    "saroglaki.christina-ioanna@ucy.ac.cy    \n",
    "jye00001@ucy.ac.cy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "As per the authors, the chosen dataset focuses on indicators associated with the diagnosis of cervical cancer, encompassing various features such as demographic information, habits, and medical records​. In more detail, the data was gathered at 'Hospital Universitario de Caracas' in Venezuela from a total of 858 patients​.\n",
    "\n",
    "C. J. Fernandes Kelwin and J. Fernandes, “Cervical cancer (Risk Factors),” UCI Machine \n",
    "Learning Repository. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df = pd.read_csv(\"risk_factors_cervical_cancer.csv\", \n",
    "            na_values=['?'])\n",
    "\n",
    "# Only keep the \"Biopsy\" column as the target variable\n",
    "risk_factor_df.drop(columns=[\"Hinselmann\",\"Schiller\",\"Citology\"], inplace=True)\n",
    "\n",
    "print(\"----------------------------------- Information -----------------------------------\")\n",
    "risk_factor_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "To gain a better understanding of the dataset we conducted a preliminary analysis. First, we needed to find the volume of missing values contained in the dataset as well as the features that contained the largest amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------- Missing Values -----------------------------------\")\n",
    "nan_columns = {}\n",
    "total_nan = 0\n",
    "total_entries = len(risk_factor_df.axes[0]) * len(risk_factor_df.axes[1])\n",
    "\n",
    "#Fidn columns containing NaN values\n",
    "for col in risk_factor_df.columns:\n",
    "    if risk_factor_df[col].isnull().any():\n",
    "        nan_in_column = risk_factor_df[col].isna().sum()\n",
    "        nan_columns[col] = nan_in_column\n",
    "        total_nan += nan_in_column\n",
    "    else:\n",
    "        nan_columns[col] = 0\n",
    "\n",
    "# Print total NaN values\n",
    "if (total_nan == 0):\n",
    "    print(\"\\nNo NaN values in the dataset.\")\n",
    "else:\n",
    "    print(\"\\nNaN values found in the dataset.\")\n",
    "    nan_columns = sorted(nan_columns.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    print(\"\\nTotal NaN values in dataset: {}/{}\".format(total_nan, total_entries))\n",
    "\n",
    "    print(\"\\nTop 15 columns with missing values:\\n\")\n",
    "    for i in range(15): print(\"{:2}. {:35} : {:}\".format(i+1, nan_columns[i][0], nan_columns[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "total_figure = px.pie(values=[total_nan, total_entries-total_nan], names=[\"NaN values\", \"Valid Values\"],\n",
    "        color_discrete_sequence=px.colors.sequential.Aggrnyl,\n",
    "        title=\"Total NaN Values Distribution\",\n",
    "        width=550, height= 350)\n",
    "total_figure.update_layout(\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    title_x=0.5    \n",
    ")\n",
    "total_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified that the features “STDs: Time since first diagnosis” and “STDs: Time since last diagnosis” were filled with NaN values of about 92%. Because of the high percentage, it was impractical to either eliminate the affected observations or fill the missing values with the mean of columns. Consequently, these features were excluded from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df.drop(columns=[\"STDs: Time since first diagnosis\", \"STDs: Time since last diagnosis\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the optimal performance of future models, we set a missing value threshold of 10 per row. Any rows that exceeded this threshold were eliminated from the dataset because we determined they lacked meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows containing NaN values\n",
    "total_rows = len(risk_factor_df)\n",
    "nan_rows = risk_factor_df.isna().any(axis=1).tolist().count(True)\n",
    "\n",
    "print(\"\\nTotal Rows containing NaN values in dataset: {}/{}\".format(nan_rows, total_rows))\n",
    "\n",
    "# Remove rows that contain more than 10 NaN values\n",
    "nan_per_row = risk_factor_df.isna().sum(axis=1)\n",
    "rows_to_del = []\n",
    "for indx in range(len(nan_per_row)-1):\n",
    "    val = nan_per_row[indx]\n",
    "    if (val > 10):\n",
    "        rows_to_del.append(indx)\n",
    "\n",
    "print(\"\\nRows containing >10 NaN values: {}/{}\".format(len(rows_to_del), total_rows))\n",
    "\n",
    "risk_factor_df.drop(rows_to_del, inplace=True)\n",
    "risk_factor_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "color_1 = [px.colors.sequential.Agsunset[0], px.colors.sequential.Agsunset[1]]\n",
    "color_2 = [px.colors.sequential.Agsunset[2], px.colors.sequential.Agsunset[3]]\n",
    "\n",
    "\n",
    "row_figure = make_subplots(1, 2, specs=[[{'type':'domain'}, {'type':'domain'}]],\n",
    "    subplot_titles=['Contain NaN Values', 'Contain >10 NaN Values'])\n",
    "\n",
    "row_figure.add_trace(go.Pie(labels=[\"Has NaN Values\",\"Is Filled\"],\n",
    "    values=[nan_rows, total_rows - nan_rows],\n",
    "    marker_colors=color_1,\n",
    "    pull=[0.1, 0]), 1, 1)\n",
    "\n",
    "row_figure.add_trace(go.Pie(labels=[\"<10 NaN\", \">10 NaN\"],\n",
    "    values=[len(rows_to_del), nan_rows - len(rows_to_del)],\n",
    "    marker_colors=color_2), 1, 2)\n",
    "\n",
    "row_figure.update_layout(title_text='Rows Containing NaN Values',\n",
    "    width=650, height= 400,\n",
    "    title_x=0.5)\n",
    "\n",
    "row_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining columns, we replaced the missing values with the mean of the existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with mean\n",
    "print(\"--------------------------- Filling Missing Values ---------------------------\")\n",
    "print(\"----------------------------------- BEFORE -----------------------------------\")\n",
    "print(\"Number of rows before filling missing values: \", len(risk_factor_df))\n",
    "\n",
    "# Display the number of missing values before filling\n",
    "print(\"\\nNumber of missing values per column before filling:\")\n",
    "print(risk_factor_df.isnull().sum())\n",
    "\n",
    "# Fill missing values with mean\n",
    "risk_factor_df.fillna(risk_factor_df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n----------------------------------- AFTER -----------------------------------\")\n",
    "print(\"Number of rows after filling missing values: \", len(risk_factor_df))\n",
    "\n",
    "# Display the number of missing values after filling\n",
    "print(\"\\nNumber of missing values per column after filling:\")\n",
    "print(risk_factor_df.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Rows\n",
    "\n",
    "Following the missing value analysis, we examined if the dataset contained any duplicate rows and removed them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------- Duplicate Rows -----------------------------------\")\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = risk_factor_df.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "\n",
    "if num_duplicates == 0:\n",
    "    print(\"No duplicate rows found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {num_duplicates} duplicate rows in the dataset.\\n\")\n",
    "\n",
    "    # Display the duplicate rows indexes (if any)\n",
    "    print(\"Duplicate rows indexes: {}\\n\".format(risk_factor_df[duplicate_rows].index.values))\n",
    "\n",
    "    # Removing duplicate rows\n",
    "    print(\"----------------------------- Removing Duplicates ----------------------------\")\n",
    "    print(\"----------------------------------- BEFORE -----------------------------------\")\n",
    "    print(\"Number of rows before removing duplicates: \", len(risk_factor_df))\n",
    "\n",
    "    # Drop duplicate rows\n",
    "    risk_factor_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    print(\"\\n----------------------------------- AFTER -----------------------------------\")\n",
    "    print(\"Number of rows after removing duplicates: \", len(risk_factor_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concluded the first part of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal dataset size: {} cols, {} rows\".format(risk_factor_df.shape[1], risk_factor_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding features\n",
    "\n",
    "Once the first part of the analysis was completed, we moved on to exploring some statistical properties of the dataset. This would allow us to identify possible connections between the features as well as possible imbalances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target value imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_0 = risk_factor_df[\"Biopsy\"].value_counts()[0]\n",
    "count_1 = risk_factor_df[\"Biopsy\"].value_counts()[1]\n",
    "\n",
    "classes_df = pd.DataFrame(\n",
    "    [[\"Healthy\",count_0], [\"Cervical Cancer\",count_1]],\n",
    "    columns =['type', 'count'])\n",
    "\n",
    "balance_fig = px.bar(classes_df, x=\"type\", y=\"count\",\n",
    "    title=\"Class Distribution\",\n",
    "    color=\"type\",\n",
    "    text_auto=True,\n",
    "    color_discrete_sequence=px.colors.qualitative.Bold,\n",
    "    width=500, height=400,\n",
    "    labels={\n",
    "        \"type\": \"Class\",\n",
    "        \"count\": \"Occurrences\"\n",
    "    })\n",
    "\n",
    "balance_fig.update_layout(\n",
    "    title_x=0.5    \n",
    ")\n",
    "\n",
    "balance_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function finding the unique values of each column in the dataframe\n",
    "def find_unique_values_df(feat: pd.DataFrame):\n",
    "    column_unique  = {}\n",
    "\n",
    "    for col in list(feat):\n",
    "        column_unique[str(col)] = feat[col].unique()\n",
    "\n",
    "    return column_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gc = risk_factor_df.iloc[:,:11].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_gc = risk_factor_df.iloc[:,:11].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------- Unique Values -----------------------------------\")    \n",
    "# Unique Values\n",
    "unique_vals = find_unique_values_df(risk_factor_df)\n",
    "\n",
    "for col in unique_vals:\n",
    "    print(\"{} : {}\".format(col, list(unique_vals[col])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mai644",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "32bdd95e8c4ecada84a0073ec4e8d048a8aaf2397f6888f3b1d4c4db30935bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
