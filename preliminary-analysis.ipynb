{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAI643 - Artificial Intelligence in Medicine\n",
    "\n",
    "Project Assignment 1 - Spring Semester 2024\n",
    "\n",
    "Student Name:    \n",
    "Christina Ioanna Saroglaki   \n",
    "Jianlin Ye \n",
    "\n",
    "UCY Email:     \n",
    "saroglaki.christina-ioanna@ucy.ac.cy    \n",
    "jye00001@ucy.ac.cy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "As per the authors, the chosen dataset focuses on indicators associated with the diagnosis of cervical cancer, encompassing various features such as demographic information, habits, and medical records​. In more detail, the data was gathered at 'Hospital Universitario de Caracas' in Venezuela from a total of 858 patients​.\n",
    "\n",
    "C. J. Fernandes Kelwin and J. Fernandes, “Cervical cancer (Risk Factors),” UCI Machine \n",
    "Learning Repository. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df = pd.read_csv(\"risk_factors_cervical_cancer.csv\")\n",
    "\n",
    "print(\"----------------------------------- Information -----------------------------------\")\n",
    "risk_factor_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset to features and target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = risk_factor_df.iloc[:,:-4]\n",
    "dep_df = risk_factor_df.iloc[:,-4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis of the dataset\n",
    "\n",
    "To gain a better understanding of the dataset we conducted a preliminary analysis. Fistly we tranformed all the numeric values into  the correct numeric type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df = risk_factor_df.apply(pd.to_numeric, errors = \"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "Next we needed to find the volume of missing values contained in the dataset as well as the features that contained the largest amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------- Missing Values -----------------------------------\")\n",
    "nan_columns = {}\n",
    "total_nan = 0\n",
    "total_entries = len(risk_factor_df.axes[0]) * len(risk_factor_df.axes[1])\n",
    "\n",
    "#Fidn columns containing NaN values\n",
    "for col in risk_factor_df.columns:\n",
    "    if risk_factor_df[col].isnull().any():\n",
    "        nan_in_column = risk_factor_df[col].isna().sum()\n",
    "        nan_columns[col] = nan_in_column\n",
    "        total_nan += nan_in_column\n",
    "    else:\n",
    "        nan_columns[col] = 0\n",
    "\n",
    "# Print total NaN values\n",
    "if (total_nan == 0):\n",
    "    print(\"\\nNo NaN values in the dataset.\")\n",
    "else:\n",
    "    print(\"\\nNaN values found in the dataset.\")\n",
    "    nan_columns = sorted(nan_columns.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(\"\\nTotal NaN values in dataset: {}/{}\".format(total_nan, total_entries))\n",
    "\n",
    "print(\"\\nAmount of NaN values per column:\")\n",
    "for sort_col in nan_columns:\n",
    "        print(\"{} : {}\".format(sort_col[0], sort_col[1]))\n",
    "\n",
    "# Rows containing NaN values\n",
    "nan_rows = risk_factor_df.iloc[:,:-4].isna().any(axis=1).tolist().count(True)\n",
    "\n",
    "print(\"\\nTotal Rows containing NaN values in dataset: {}/{}\".format(nan_rows, len(risk_factor_df)))\n",
    "\n",
    "# Plots\n",
    "total_labels = [\"NaN values\", \"Valid Values\"]\n",
    "total_size = [total_nan, total_entries-total_nan]\n",
    "\n",
    "row_labels = [\"Contain NaN\", \"FIlled rows\"]\n",
    "row_size = [nan_rows, len(risk_factor_df)]\n",
    "\n",
    "fig_1, ax_1 = plt.subplots(figsize=(10, 3), subplot_kw=dict(aspect=\"equal\"))\n",
    "ax_1.pie(total_size, labels=total_labels, autopct='%1.1f%%', textprops=dict(color=\"w\"))\n",
    "ax_1.legend(loc= \"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "ax_1.set_title(\"Total Missing Values\")\n",
    "\n",
    "fig_2, ax_2 = plt.subplots(figsize=(10, 3), subplot_kw=dict(aspect=\"equal\"))\n",
    "ax_2.pie(row_size, labels=row_labels, autopct='%1.1f%%', textprops=dict(color=\"w\"))\n",
    "ax_2.legend(loc= \"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "ax_2.set_title(\"Total Rows Containing Missing Values\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified that the features “STDs: Time since first diagnosis” and “STDs: Time since last diagnosis” were filled with NaN values to about 92%. Because of the high percentage of missing values, it impractical to either eliminate those observations or fill the missing data with the mean of the existing data. Consequently, these features were excluded from the dataset for the development of the models.\n",
    "\n",
    "For the remaining columns, we can replace the missing values with the mean of the existing data during the pre-processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df.drop(columns=[\"STDs: Time since first diagnosis\", \"STDs: Time since last diagnosis\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate rows\n",
    "\n",
    "Following the missing value analysis, we examined if the dataset contained any duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = risk_factor_df.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "\n",
    "if num_duplicates == 0:\n",
    "    print(\"No duplicate rows found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {num_duplicates} duplicate rows in the dataset.\")\n",
    "\n",
    "# Display the duplicate rows (if any)\n",
    "if num_duplicates > 0:\n",
    "    duplicate_rows_df = risk_factor_df[duplicate_rows]\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    print(duplicate_rows_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function finding the unique values of each column in the dataframe\n",
    "def find_unique_values_df(feat: pd.DataFrame):\n",
    "    column_unique  = {}\n",
    "\n",
    "    for col in list(feat):\n",
    "        column_unique[str(col)] = feat[col].unique()\n",
    "\n",
    "    return column_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------- Unique Values -----------------------------------\")    \n",
    "# Unique Values\n",
    "unique_vals = find_unique_values_df(risk_factor_df)\n",
    "\n",
    "for col in unique_vals:\n",
    "    print(\"\\n{} : {}\".format(col, unique_vals[col]))\n",
    "\n",
    "    #Convert all columns to contain numerical values\n",
    "    risk_factor_df[col] = risk_factor_df[col].apply(pd.to_numeric, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows\n",
    "print(\"----------------------------- Removing Duplicates ----------------------------\")\n",
    "print(\"----------------------------------- BEFORE -----------------------------------\")\n",
    "print(\"Number of rows before removing duplicates: \", len(risk_factor_df))\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = risk_factor_df.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "\n",
    "if num_duplicates == 0:\n",
    "    print(\"No duplicate rows found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {num_duplicates} duplicate rows in the dataset.\")\n",
    "\n",
    "# Display the duplicate rows (if any)\n",
    "if num_duplicates > 0:\n",
    "    duplicate_rows_df = risk_factor_df[duplicate_rows]\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    print(duplicate_rows_df)\n",
    "\n",
    "# Drop duplicate rows\n",
    "risk_factor_df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"----------------------------------- AFTER -----------------------------------\")\n",
    "print(\"Number of rows after removing duplicates: \", len(risk_factor_df))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mai644",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "32bdd95e8c4ecada84a0073ec4e8d048a8aaf2397f6888f3b1d4c4db30935bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
