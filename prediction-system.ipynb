{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAI643 - Artificial Intelligence in Medicine\n",
    "\n",
    "Project Assignment 1 - Spring Semester 2024\n",
    "\n",
    "Student Name:    \n",
    "Christina Ioanna Saroglaki   \n",
    "Jianlin Ye \n",
    "\n",
    "UCY Email:     \n",
    "saroglaki.christina-ioanna@ucy.ac.cy    \n",
    "jye00001@ucy.ac.cy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description \n",
    "----\n",
    "This file contains the source code for the development of the cervical cancer prediction system.\n",
    "\n",
    "**This does not contain the source code for the preliminary analysis.** The source code for the preliminary analysis is contained in the `pre-processing.ipynb` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install pyarrow imbalanced-learn\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "np.set_printoptions(formatter={'float':\"{:6.5g}\".format})\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "As per the authors, the chosen dataset focuses on indicators associated with the diagnosis of cervical cancer, encompassing various features such as demographic information, habits, and medical records​. In more detail, the data was gathered at \"Hospital Universitario de Caracas\" in Venezuela from a total of 858 patients​.\n",
    "\n",
    "C. J. Fernandes Kelwin and J. Fernandes, “Cervical cancer (Risk Factors),” UCI Machine \n",
    "Learning Repository. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df = pd.read_csv(\"risk_factors_cervical_cancer.csv\", \n",
    "            na_values=[\"?\"])\n",
    "\n",
    "print(\"----------------------------------- Information -----------------------------------\")\n",
    "risk_factor_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing steps\n",
    "---\n",
    "- Re-encoded missing values from \"?\" to `NaN`\n",
    "- Handled missing and duplicate values\n",
    "- Dropped zero variance features\n",
    "- Removed outliers\n",
    "- Split data into training, validation and test sets\n",
    "- Handled target variable imbalance\n",
    "- Performed dimensionality reduction\n",
    "\n",
    "## Data cleaning\n",
    "### Missing Values\n",
    "\n",
    "First, we needed to find and manage the volume of missing values contained in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------- Missing Values -----------------------------------\")\n",
    "missing_info = risk_factor_df.isnull().sum()\n",
    "total_nan = missing_info.sum()\n",
    "total_entries = risk_factor_df.size\n",
    "\n",
    "# Print total NaN values\n",
    "if (total_nan == 0):\n",
    "    print(\"\\nNo NaN values in the dataset.\")\n",
    "else:\n",
    "    print(\"\\nNaN values found in the dataset.\")\n",
    "\n",
    "    print(\"\\nTotal NaN values in dataset: {}/{}\".format(total_nan, total_entries))\n",
    "\n",
    "    # Sort columns by the number of missing values\n",
    "    nan_columns = missing_info.sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nTop 15 columns with missing values:\\n\")\n",
    "    for i, (col, count) in enumerate(nan_columns.head(15).items(), 1):\n",
    "        print(\"{:2}. {:35} : {:}\".format(i, col, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified that the features `STDs: Time since first diagnosis` and `STDs: Time since last diagnosis` were filled with NaN values of about 92%. Because of the high percentage, it was impractical to either eliminate the affected observations or fill the missing values with the mean of columns. Consequently, these features were excluded from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factor_df.drop(columns=[\"STDs: Time since first diagnosis\", \"STDs: Time since last diagnosis\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the optimal performance of future models, we also set a **missing value threshold of 10 per row**. Any rows that exceeded this threshold were eliminated from the dataset because we determined they were missing significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows containing NaN values\n",
    "total_rows = len(risk_factor_df)\n",
    "nan_rows = risk_factor_df.isna().any(axis=1).tolist().count(True)\n",
    "print(\"\\nTotal Rows containing NaN values in dataset: {}/{}\".format(nan_rows, total_rows))\n",
    "\n",
    "# Find rows that contain more than 10 NaN values\n",
    "rows_to_del = risk_factor_df[risk_factor_df.isna().sum(axis=1) > 10].index\n",
    "\n",
    "print(\"\\nRows containing >10 NaN values: {}/{}\".format(len(rows_to_del), total_rows))\n",
    "\n",
    "# Remove rows\n",
    "risk_factor_df.drop(rows_to_del, inplace=True)\n",
    "risk_factor_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining columns, we managed the missing values depending on the column. In more detail, if the column contained binary values (0,1) then the row containing the missing value was deleted. Otherwise, the missing value was replaced with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------- Handling Missing Values ---------------------------\")\n",
    "print(\"----------------------------------- BEFORE -----------------------------------\")\n",
    "print(\"Number of rows before filling missing values: \", len(risk_factor_df))\n",
    "\n",
    "# Display the number of missing values before filling\n",
    "print(\"\\nNumber of missing values per column before filling:\")\n",
    "print(risk_factor_df.isnull().sum())\n",
    "\n",
    "# Fill missing values depending on the column\n",
    "for col in risk_factor_df.columns:\n",
    "    # If the column has more than 3 unique values, fill with mean of the column\n",
    "    if risk_factor_df[col].nunique() > 3:\n",
    "        risk_factor_df[col] = risk_factor_df[col].fillna(risk_factor_df[col].median())\n",
    "    \n",
    "# Drop rest NaN containing rows\n",
    "risk_factor_df=risk_factor_df.dropna()\n",
    "risk_factor_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n----------------------------------- AFTER -----------------------------------\")\n",
    "print(\"Number of rows after filling missing values: \", len(risk_factor_df))\n",
    "\n",
    "# Display the number of missing values after filling\n",
    "print(\"\\nNumber of missing values per column after filling:\")\n",
    "print(risk_factor_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Rows\n",
    "\n",
    "Following the missing value analysis, we examined if the dataset contained any duplicate rows and removed them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------- Duplicate Rows -----------------------------------\")\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = risk_factor_df.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "\n",
    "if num_duplicates == 0:\n",
    "    print(\"No duplicate rows found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Found {num_duplicates} duplicate rows in the dataset.\\n\")\n",
    "\n",
    "    # Display the duplicate rows indexes (if any)\n",
    "    print(\"Duplicate rows indexes: {}\\n\".format(risk_factor_df[duplicate_rows].index.values))\n",
    "\n",
    "    # Removing duplicate rows\n",
    "    print(\"----------------------------- Removing Duplicates ----------------------------\")\n",
    "    print(\"----------------------------------- BEFORE -----------------------------------\")\n",
    "    print(\"Number of rows before removing duplicates: \", len(risk_factor_df))\n",
    "\n",
    "    risk_factor_df.drop_duplicates(inplace=True)\n",
    "    risk_factor_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"\\n----------------------------------- AFTER -----------------------------------\")\n",
    "    print(\"Number of rows after removing duplicates: \", len(risk_factor_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove zero variance features\n",
    "MOving on, we also calculated the mean and standard deviation for each column. Columns with a standard deviation of 0 were omitted from the dataset because they did not add significant variability to the data since they contained the same value for all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = risk_factor_df.mean()\n",
    "std_df = risk_factor_df.std()\n",
    "\n",
    "# Print columns that have a standard deviation 0 (contain only one value)\n",
    "print(\"Columns containing 1 value: {}\\n\".format(std_df[std_df==0].index.values))\n",
    "\n",
    "risk_factor_df.drop(columns=[\"STDs:cervical condylomatosis\", \"STDs:AIDS\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers\n",
    "\n",
    "The IQR (Inter Quartile Range) approach is the most commonly used and most trusted approach used in the research field to find outliers in a dataset. We utilised IQR to identify and remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function finding the unique values of each column in the dataframe\n",
    "def find_unique_values_df(feat: pd.DataFrame):\n",
    "    return {col: feat[col].unique() for col in feat}\n",
    "\n",
    "def find_outliers(col, indices):\n",
    "    obs = risk_factor_df[col].iloc[indices]\n",
    "    unique_items, counts = np.unique(obs, return_counts=True)\n",
    "    unique_items, counts = unique_items[::-1], counts[::-1]\n",
    "\n",
    "    values_to_delete = unique_items[counts < 2 ]\n",
    "    return values_to_delete\n",
    "\n",
    "def delete_outliers(col, to_delete):\n",
    "    if (to_delete.size != 0):\n",
    "        rows_to_del = risk_factor_df.loc[risk_factor_df[col].isin(to_delete)].index.values.tolist()\n",
    "\n",
    "        # Remove rows\n",
    "        risk_factor_df.drop(rows_to_del, inplace=True)\n",
    "        risk_factor_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Values\n",
    "unique_vals = find_unique_values_df(risk_factor_df)\n",
    "# Identify non-binary columns\n",
    "non_binary_cols = [col for col, vals in unique_vals.items() if len(vals) > 2]\n",
    "\n",
    "for col in non_binary_cols:\n",
    "\n",
    "    # IQR cannot be applied to columns with median 0\n",
    "    if (risk_factor_df[col].median() != 0):\n",
    "        Q3, Q1 = np.percentile(risk_factor_df[col], [75 ,25])\n",
    "        IQR = Q3-Q1\n",
    "\n",
    "        upper = Q3+(1.5*IQR)\n",
    "        lower = Q1-(1.5*IQR)\n",
    "\n",
    "        print(col)\n",
    "        print(\"median: {}, upper fence: {}, lower fence: {}\\n\".format(risk_factor_df[col].median(), upper, lower))\n",
    "\n",
    "        #Delete one occurrence observations outside the upper fence as outliers\n",
    "        upper_to_delete = find_outliers(col, np.where(risk_factor_df[col] > upper)[0])\n",
    "        delete_outliers(col, upper_to_delete)\n",
    "\n",
    "        \n",
    "        #Delete one occurrence observations outside the lower fence as outliers\n",
    "        lower_to_delete = find_outliers(col, np.where(risk_factor_df[col] < lower)[0])\n",
    "        delete_outliers(col, lower_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal dataset size: {} cols, {} rows\".format(risk_factor_df.shape[1], risk_factor_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset splitting\n",
    "\n",
    "### Target Variable\n",
    "\n",
    "As previously stated, the final system will focus on predicting a single target variable. According to the literature, Pap tests primarily serve as preventative medical screenings, while the Schiller and colposcopy examinations are usually coupled with a biopsy to validate the results of the tests. Considering this literature and our prior analysis, we have selected the `Biopsy` feature as the target variable for our system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = risk_factor_df.drop(columns=[\"Hinselmann\", \"Schiller\", \"Citology\", \"Biopsy\"])\n",
    "y = risk_factor_df[\"Biopsy\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify = y, \n",
    "                                                    shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, \n",
    "                                                y_temp, \n",
    "                                                test_size=0.5,\n",
    "                                                random_state=42,\n",
    "                                                stratify = y_temp,\n",
    "                                                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Data\n",
    "In order to address the imbalance within the target variable, we implemented the **SMOTE-ENN** method. Introduced by Batista et al. (2004), this method combines the ability of SMOTE to generate synthetic data points for the minority class, and ENN (Edited Nearest Neighbor), which eliminates data points from both classes that have a different class from the majority class among their K-Nearest Neighbors.\n",
    "\n",
    "We opted for SMOTE-ENN over SMOTETomek due to its ability to remove noisy samples, thereby enhancing the quality of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_enn = SMOTEENN(random_state=42)\n",
    "\n",
    "x_train_sm, y_train_sm = smote_enn.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "color_1 = [px.colors.qualitative.Prism[0], px.colors.qualitative.Prism[1]]\n",
    "\n",
    "y_train_counts = y_train.value_counts().reset_index()\n",
    "y_train_counts.columns = ['Class', 'Count']\n",
    "y_train_counts['Class'] = y_train_counts['Class'].map({0: 'Healthy', 1: 'Cervical Cancer'})\n",
    "\n",
    "y_train_sm_counts = y_train_sm.value_counts().reset_index()\n",
    "y_train_sm_counts.columns = ['Class', 'Count']\n",
    "y_train_sm_counts['Class'] = y_train_sm_counts['Class'].map({0: 'Healthy', 1: 'Cervical Cancer'})\n",
    "\n",
    "targ_fig = make_subplots(1, 2, specs=[[{\"type\":\"domain\"}, {\"type\":\"domain\"}]],\n",
    "    subplot_titles=[\"Original Distribution\", \"Balanced Distribution\"])\n",
    "\n",
    "targ_fig.add_trace(go.Pie(labels=y_train_counts[\"Class\"],\n",
    "    values=y_train_counts[\"Count\"],\n",
    "    marker_colors=color_1), 1, 1)\n",
    "\n",
    "targ_fig.add_trace(go.Pie(labels=y_train_sm_counts[\"Class\"],\n",
    "    values=y_train_sm_counts[\"Count\"],\n",
    "    marker_colors=color_1), 1, 2)\n",
    "\n",
    "targ_fig.update_layout(title_text=\"Rows Containing NaN Values\",\n",
    "    width=850, height= 400,\n",
    "    title_x=0.5)\n",
    "\n",
    "targ_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling and Encoding\n",
    "Scaling and Encoding were only performed in the training set to avoid information leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Specify the features to be scaled\n",
    "features_to_scale = [\"Smokes (packs/year)\", \"Hormonal Contraceptives (years)\", \"IUD (years)\"]\n",
    "\n",
    "# Scale the specified features and add new columns with suffix '_scaled'\n",
    "x_train_sm[[f\"{feature}_scaled\" for feature in features_to_scale]] = scaler.fit_transform(x_train_sm[features_to_scale])\n",
    "\n",
    "# Transform the scaled columns in the validation and test sets as well\n",
    "X_val[[f\"{feature}_scaled\" for feature in features_to_scale]] = scaler.transform(X_val[features_to_scale])\n",
    "X_test[[f\"{feature}_scaled\" for feature in features_to_scale]] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "#Drop original columns from the training set\n",
    "x_train_sm.drop(columns=features_to_scale, inplace=True)\n",
    "X_val.drop(columns=features_to_scale, inplace=True)\n",
    "X_test.drop(columns=features_to_scale, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age categories, every 5 years as an intervals\n",
    "age_intervals = [13, 18, 23, 28, 33, 38, 43, 48, 53, 58, 63, 68, 73, 78, 83, 88]\n",
    "labels = list(range(len(age_intervals) - 1))\n",
    "\n",
    "# Encode the \"Age\" feature\n",
    "x_train_sm['Age_encoded'] = pd.cut(x_train_sm['Age'], bins=age_intervals, labels=labels, right=False)\n",
    "X_val['Age_encoded'] = pd.cut(X_val['Age'], bins=age_intervals, labels=labels, right=False)\n",
    "X_test['Age_encoded'] = pd.cut(X_test['Age'], bins=age_intervals, labels=labels, right=False)\n",
    "\n",
    "#Drop original column from the training set\n",
    "x_train_sm.drop(columns=['Age'], inplace=True)\n",
    "X_val.drop(columns=['Age'], inplace=True)\n",
    "X_test.drop(columns=['Age'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "minMaxScaler = MinMaxScaler()\n",
    "x_train_rescaled = minMaxScaler.fit_transform(x_train_sm)\n",
    "x_val_rescaled = minMaxScaler.transform(X_val)\n",
    "x_test_rescaled = minMaxScaler.transform(X_test)\n",
    "\n",
    "# 95% of variance\n",
    "pca = PCA(n_components=11)\n",
    "x_train_reduced = pca.fit_transform(x_train_rescaled)\n",
    "x_val_reduced = pca.transform(x_val_rescaled)\n",
    "x_test_reduced = pca.transform(x_test_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "---\n",
    "For the prediction models we selected to utilize the models of Support Vector Machine (SVM) and the ensemble method of Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mai644",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "32bdd95e8c4ecada84a0073ec4e8d048a8aaf2397f6888f3b1d4c4db30935bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
